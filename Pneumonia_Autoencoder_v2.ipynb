{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ub5cg7ZnzBn7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import gc\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAvgPool2D,Conv2D, MaxPool2D, BatchNormalization, Dropout, Input, LeakyReLU, Reshape, Conv2DTranspose, Concatenate, Lambda, MaxPooling2D\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        " \n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "UnKcRVzAz6CR",
        "outputId": "d34f283a-4527-4a01-9a4c-831bc0d35c44"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-94c949e0-0356-49ef-ad1a-25c74cd42259\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-94c949e0-0356-49ef-ad1a-25c74cd42259\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading chest-xray-pneumonia.zip to /content\n",
            "100% 2.29G/2.29G [01:46<00:00, 23.5MB/s]\n",
            "100% 2.29G/2.29G [01:46<00:00, 23.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Kaggle API Token\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Setup Kaggle API\n",
        "!pip install kaggle --quiet\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the Dataset\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        " # Extract the Dataset\n",
        "!unzip -q chest-xray-pneumonia.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the directory path for training and testing images\n",
        "base_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Function to copy files from source directories to a target directory\n",
        "def copy_files(source_dir, target_dir, num_files=None):\n",
        "   count = 0\n",
        "   for filename in os.listdir(source_dir):\n",
        "      if num_files is not None and count >= num_files:\n",
        "         break\n",
        "      file_path = os.path.join(source_dir, filename)\n",
        "      if os.path.isfile(file_path):\n",
        "         count += 1\n",
        "         shutil.copy(file_path, target_dir)\n",
        "\n",
        "   print(f'Copied {count} files from {source_dir} to {target_dir}')\n",
        "\n",
        "# Set up the directory path for training, validation, and testing images\n",
        "base_dir = './chest_xray'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'test')\n",
        "test_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "\n",
        "# Create a \"train_undersampled\" directory to store the undersampled training images\n",
        "train_undersampled = os.path.join(base_dir, 'train_undersampled')\n",
        "os.makedirs(train_undersampled, exist_ok=True)\n",
        "# Create new 'NORMAL' and 'PNEUMONIA' directories inside the train_undersampled directory\n",
        "normal_dir_undersampled = os.path.join(train_undersampled, 'NORMAL')\n",
        "pneumonia_dir_undersampled = os.path.join(train_undersampled, 'PNEUMONIA')\n",
        "os.makedirs(normal_dir_undersampled, exist_ok=True)\n",
        "os.makedirs(pneumonia_dir_undersampled, exist_ok=True)\n",
        "# Copy images from the training 'NORMAL' and 'PNEUMONIA' directories to the 'train_undersampled' directory in a 1:1 ratio\n",
        "# Calculate the number of files in the 'NORMAL' and 'PNEUMONIA' directories\n",
        "normal_count = len(os.listdir(f'{train_dir}/NORMAL'))\n",
        "pneumonia_count = len(os.listdir(f'{train_dir}/PNEUMONIA'))\n",
        "minority_count = min(normal_count, pneumonia_count)\n",
        "# Copy minority_count number of images from 'train_dir' to 'train_undersampled'\n",
        "copy_files(os.path.join(train_dir, 'NORMAL'), normal_dir_undersampled, minority_count)\n",
        "copy_files(os.path.join(train_dir, 'PNEUMONIA'), pneumonia_dir_undersampled, minority_count)\n",
        "\n",
        "\n",
        "# Create a 'val_undersampled' directory to store the undersampled validation images\n",
        "val_undersampled = os.path.join(base_dir, 'val_undersampled')\n",
        "os.makedirs(val_undersampled, exist_ok=True)\n",
        "# Create new 'NORMAL' and 'PNEUMONIA' directories inside the val_undersampled directory\n",
        "normal_dir_undersampled = os.path.join(val_undersampled, 'NORMAL')\n",
        "pneumonia_dir_undersampled = os.path.join(val_undersampled, 'PNEUMONIA')\n",
        "os.makedirs(normal_dir_undersampled, exist_ok=True)\n",
        "os.makedirs(pneumonia_dir_undersampled, exist_ok=True)\n",
        "# Copy images from the validation 'NORMAL' and 'PNEUMONIA' directories to the 'val_undersampled' directory in a 1:1 ratio\n",
        "# Calculate the number of files in the 'NORMAL' and 'PNEUMONIA' directories\n",
        "normal_count = len(os.listdir(f'{val_dir}/NORMAL'))\n",
        "pneumonia_count = len(os.listdir(f'{val_dir}/PNEUMONIA'))\n",
        "minority_count = min(normal_count, pneumonia_count)\n",
        "# Copy minority_count number of images from 'val_dir' to 'val_undersampled'\n",
        "copy_files(os.path.join(val_dir, 'NORMAL'), normal_dir_undersampled, minority_count)\n",
        "copy_files(os.path.join(val_dir, 'PNEUMONIA'), pneumonia_dir_undersampled, minority_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reRUWXoRzBn9",
        "outputId": "f9a30435-0738-4c20-8aea-b087fa8be347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Preprocessing the images - setting up image data generators\n",
        "batch_size = 24\n",
        "image_size = 224\n",
        "\n",
        "# Limited data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,\n",
        "                           width_shift_range=0.2, height_shift_range=0.2)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "combined_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "balanced_train_generator = train_datagen.flow_from_directory(\n",
        "      train_undersampled,\n",
        "      target_size=(image_size, image_size),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='binary',\n",
        "      shuffle=True\n",
        ")\n",
        "\n",
        "balanced_val_generator = val_datagen.flow_from_directory(\n",
        "      val_undersampled,\n",
        "      target_size=(image_size, image_size),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='binary',\n",
        "      shuffle=True\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "      test_dir,\n",
        "      target_size=(image_size, image_size),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='binary',\n",
        "      shuffle=False\n",
        ")\n",
        "\n",
        "# Use tf.data.Dataset for efficient data loading\n",
        "def generator_wrapper(generator):\n",
        "   for x_batch, _ in generator:\n",
        "      x_batch = tf.cast(x_batch, tf.float32)\n",
        "      yield (x_batch, x_batch)\n",
        "# Shuffle buffer size\n",
        "buffer_size = 2\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "   lambda: generator_wrapper(balanced_train_generator),\n",
        "   output_signature=(\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32),\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32))\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "   lambda: generator_wrapper(balanced_val_generator),\n",
        "   output_signature=(\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32),\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32))\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "   lambda: generator_wrapper(test_generator),\n",
        "   output_signature=(\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32),\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32))\n",
        ").prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Encoder\n",
        "conv_encoder = Sequential([\n",
        "         Reshape([image_size, image_size, 3], name=\"input\"), # input: 224x224x1\n",
        "         BatchNormalization(),\n",
        "         Conv2D(16, 3, padding=\"same\", activation=\"relu\"), \n",
        "         MaxPool2D(pool_size=2), # output: 112x112x16\n",
        "         Dropout(0.15),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 56x56x32\n",
        "         Dropout(0.35),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 28x28x64\n",
        "         Dropout(0.55),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(128, 5, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 14x14x128\n",
        "         Dropout(0.35),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(256, 5, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 7x7x256\n",
        "         Dropout(0.15),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(512, 5, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 3x3x512\n",
        "         Dropout(0.15),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(90, 5, padding=\"same\", activation=\"relu\"),\n",
        "         GlobalAvgPool2D(), # output: 1x1x90\n",
        "      ])\n",
        "\n",
        "# Decoder\n",
        "conv_decoder = Sequential([\n",
        "         Dense(28 * 28 * 512),\n",
        "         Reshape((28, 28, 512), name=\"decoder_input\"), # input: 28x28x512\n",
        "         Conv2DTranspose(512, 5, strides=2, padding=\"same\", activation=\"relu\"), # output: 56x56x512\n",
        "         Dropout(0.15),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(256, 5, strides=2, padding=\"same\", activation=\"relu\"), # output: 112x112x256\n",
        "         Dropout(0.25),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(128, 5, strides=2, padding=\"same\", activation=\"relu\"), # output: 224x224x128\n",
        "         Dropout(0.5),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(64, 3, strides=1, padding=\"same\", activation=\"relu\"), # output: 224x224x64\n",
        "         Dropout(0.25),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(32, 3, strides=1, padding=\"same\", activation=\"relu\"), # output: 224x224x32\n",
        "         Dropout(0.15),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(16, 3, strides=1, padding=\"same\", activation=\"relu\"), # output: 224x224x16\n",
        "         Dropout(0.05),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(3, 3, strides=1, padding=\"same\", activation=\"sigmoid\"), # output: 224x224x1\n",
        "         Reshape([image_size, image_size, 3], name=\"output\")\n",
        "      ])\n",
        "\n",
        "# Autoencoder\n",
        "conv_ae = Sequential([conv_encoder, conv_decoder])\n",
        "\n",
        "\n",
        "conv_ae.compile(\n",
        "    loss=\"mse\", \n",
        "    # Use nadam with a 0.01 learning rate\n",
        "    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.05),\n",
        "     metrics=[tf.keras.metrics.MeanSquaredError(),'accuracy'])\n",
        "\n",
        "conv_ae.build(input_shape=(None, image_size, image_size, 3))\n",
        "conv_ae.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MNIST AE example from the Hands-On Machine Learning text reduced the input rank from 28x28x1 = 784 to 30, a 96.17% reduction. We are attempting to reduce our input rank from 224x224x3 = 150,528 to around 100, a 99.93% reduction. To make that difference more apparent, if we were to reduce rank by 96.17% on our input, we would have a bottleneck tensor rank of 5765 (150,528 * 0.0383). This is a significant difference in the amount of information we are trying to compress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the callbacks\n",
        "ae_early_stopping = EarlyStopping(monitor='mean_squared_error', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
        "ae_model_checkpoint = ModelCheckpoint('conv_ae.keras', monitor='mean_squared_error', verbose=1,mode='min', save_best_only=True)\n",
        "ae_reduce_lr = ReduceLROnPlateau(monitor='mean_squared_error', factor=0.01, patience=4, verbose=1, mode='min')\n",
        "\n",
        "# Define the callbacks for the classifier (monitor validation accuracy)\n",
        "classifier_early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='max', restore_best_weights=True)\n",
        "classifier_model_checkpoint = ModelCheckpoint('enhanced_clf_v1.keras', monitor='val_accuracy', verbose=1, mode='max', save_best_only=True)\n",
        "classifier_reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.01, patience=4, verbose=1, mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "history = conv_ae.fit(\n",
        "   train_dataset,\n",
        "   epochs=30,\n",
        "   #validation_data=test_dataset,\n",
        "   steps_per_epoch=len(balanced_train_generator),\n",
        "   #validation_steps=len(test_generator),\n",
        "   callbacks=[\n",
        "      ae_early_stopping,\n",
        "      ae_model_checkpoint,\n",
        "      ae_reduce_lr\n",
        "   ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the training history using plotly\n",
        "\n",
        "def plot_history(history):\n",
        "   # Plot the mean squared error and accuracy\n",
        "   fig = px.line(\n",
        "      history.history,\n",
        "      y=['mean_squared_error', 'accuracy'],\n",
        "      \n",
        "      labels={'index': 'epoch', 'value': 'mean squared error'},\n",
        "      title='Autoencoder Training History (90 filters, balanced train set only)'\n",
        "   )\n",
        "   fig.show()\n",
        "\n",
        "plot_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to visualize the encoded images\n",
        "def visualize_encoded_images(model, dataset, num_images=5):\n",
        "   # Get a batch of images from the dataset\n",
        "   for x_batch, _ in dataset.take(1):\n",
        "      encoded_images = model.layers[0](x_batch)\n",
        "      decoded_images = model.layers[1](encoded_images)\n",
        "      break\n",
        "\n",
        "   # Plot the original and decoded images\n",
        "   fig, axs = plt.subplots(2, num_images, figsize=(20, 5))\n",
        "   for i in range(num_images):\n",
        "      axs[0, i].imshow(x_batch[i])\n",
        "      axs[0, i].axis('off')\n",
        "      axs[1, i].imshow(decoded_images[i])\n",
        "      axs[1, i].axis('off')\n",
        "      fig.suptitle('Original (top) vs. Decoded (bottom) Images (90 filters, balanced train set only)')\n",
        "\n",
        "   plt.show()\n",
        "\n",
        "visualize_encoded_images(conv_ae, test_dataset) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Pneumonia Detection using Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the classifier as a Functional model, including a block of convolutional layers, then the encoder, then a block of dense layers\n",
        "def build_classifier(encoder):\n",
        "   # Freeze the encoder layers\n",
        "   for layer in encoder.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "   # Create an input layer that contains two inputs: the encoder input and a shallow input\n",
        "   input_= Input((image_size, image_size, 3), name='input')\n",
        "\n",
        "   # Using encoder as the sole feature extractor\n",
        "   encoded_features = encoder(input_)\n",
        "   x = BatchNormalization(name=\"encoder_batchNorm_1\")(encoded_features)\n",
        "   DNN_output = Dense(1, activation='sigmoid', name='output')(x)\n",
        "\n",
        "   # Create the model\n",
        "   model = Model(inputs=input_, outputs=DNN_output)\n",
        "\n",
        "   return model\n",
        "\n",
        "# Build the classifier\n",
        "classifier = build_classifier(conv_encoder)\n",
        "classifier.compile(\n",
        "   loss='binary_crossentropy',\n",
        "   optimizer=tf.keras.optimizers.Nadam(learning_rate=0.05),\n",
        "   metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "# SHow the block diagram of the classifier using plot_model\n",
        "plot_model(classifier, show_shapes=True, show_layer_names=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the classifier\n",
        "history = classifier.fit(\n",
        "   balanced_train_generator,\n",
        "   validation_data=balanced_val_generator,\n",
        "   epochs=15,\n",
        "   steps_per_epoch=len(balanced_train_generator),\n",
        "   validation_steps=len(balanced_val_generator),\n",
        "   callbacks=[classifier_early_stopping, classifier_model_checkpoint, classifier_reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the classifier\n",
        "classifier.evaluate(test_generator, steps=len(test_generator))\n",
        "\n",
        "# Get the true labels\n",
        "y_true = test_generator.classes\n",
        "# Get the predicted labels\n",
        "y_pred = classifier.predict(test_generator)\n",
        "# Round the predicted labels up and down\n",
        "y_pred = np.round(y_pred)\n",
        "# Print the classification report\n",
        "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n",
        "\n",
        "# Plot the training history using plotly\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines', name='Train'))\n",
        "fig.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines', name='Validation'))\n",
        "fig.add_trace(go.Scatter(y=history.history['auc'], mode='lines', name='AUC'))\n",
        "fig.update_layout(title=f'Classifier Accuracy (balanced train set only)', xaxis_title='Epochs', yaxis_title='Accuracy')\n",
        "fig.show()\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=history.history['loss'], mode='lines', name='Train'))\n",
        "fig.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines', name='Validation'))\n",
        "fig.update_layout(title='Classifier Loss (balanced train set only)', xaxis_title='Epochs', yaxis_title='Loss')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the predictions images and labels from the classifier\n",
        "predictions = np.round(classifier.predict(test_generator, steps=len(test_generator)))\n",
        "example_true_labels = test_generator.classes\n",
        "plt.figure(figsize=(10, 10))\n",
        "# plot the first 6 images in subplots with two columns\n",
        "for i in range(6):\n",
        "   plt.subplot(3, 2, i + 1)\n",
        "   img = next(test_generator)[0][0]\n",
        "   plt.imshow(img)\n",
        "   plt.axis('off')\n",
        "   plt.title(f'Prediction: {predictions[i][0]}, Actual: {example_true_labels[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VAE for X-Ray Image Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a VAE model for image generation\n",
        "\n",
        "class Sampling(tf.keras.layers.Layer): \n",
        "   def call(self, inputs):\n",
        "      mean, log_var = inputs\n",
        "      return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + mean\n",
        "\n",
        "codings_size = 90\n",
        "inputs = Input(shape=[224, 224, 3])\n",
        "Z = Flatten()(inputs)\n",
        "Z = Dense(150, activation=\"relu\")(Z)\n",
        "Z = Dense(100, activation=\"relu\")(Z) \n",
        "codings_mean = Dense(codings_size)(Z) # μ \n",
        "codings_log_var = Dense(codings_size)(Z) # γ \n",
        "codings = Sampling()([codings_mean, codings_log_var]) \n",
        "\n",
        "variational_encoder = Model(inputs=[inputs], outputs=[codings_mean, codings_log_var, codings], name=\"variational_encoder\")\n",
        "\n",
        "decoder_inputs = Input(shape=[codings_size])\n",
        "x = Dense(100, activation=\"relu\")(decoder_inputs)\n",
        "x = Dense(150, activation=\"relu\")(x)\n",
        "x = Dense(224 * 224 * 3)(x)\n",
        "outputs = Reshape([224, 224, 3])(x)\n",
        "variational_decoder = Model(inputs=[decoder_inputs], outputs=[outputs], name=\"variational_decoder\")\n",
        "\n",
        "_, _, codings = variational_encoder(inputs)\n",
        "reconstructions = variational_decoder(codings)\n",
        "variational_ae = Model(inputs=[inputs], outputs=[reconstructions], name=\"variational_ae\")\n",
        "\n",
        "latent_loss = 90 * tf.reduce_sum(1 + codings_log_var - tf.exp(codings_log_var) - tf.square(codings_mean), axis=-1)\n",
        "print(latent_loss.shape)\n",
        "variational_ae.add_loss(tf.reduce_mean(latent_loss) / 150528)\n",
        "\n",
        "variational_ae.compile(\n",
        "   loss= \"mean_squared_error\",\n",
        "   optimizer=tf.keras.optimizers.Nadam(learning_rate=0.1),\n",
        "   metrics=['accuracy', 'AUC', tf.keras.metrics.MeanSquaredError()]\n",
        ")\n",
        "\n",
        "variational_ae.build(input_shape=(None, image_size, image_size, 3))\n",
        "\n",
        "\n",
        "# Train the variational autoencoder\n",
        "history = variational_ae.fit(\n",
        "   train_dataset,\n",
        "   validation_data=val_dataset,\n",
        "   epochs=15,\n",
        "   steps_per_epoch=len(balanced_train_generator),\n",
        "   validation_steps=len(balanced_val_generator),\n",
        "   callbacks=[classifier_early_stopping, classifier_model_checkpoint, classifier_reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate images using the VAE\n",
        "def generate_images(model, num_images=5):\n",
        "   # Generate random codings\n",
        "   codings = tf.random.normal(shape=[num_images, codings_size])\n",
        "   # Decode the codings\n",
        "   decoded_images = model.layers[1](codings)\n",
        "   # Plot the decoded images\n",
        "   fig, axs = plt.subplots(1, num_images, figsize=(20, 5))\n",
        "   for i in range(num_images):\n",
        "      axs[i].imshow(decoded_images[i])\n",
        "      axs[i].axis('off')\n",
        "      fig.suptitle(f'Generated Images (codings size: {codings_size}, balanced train set only)')\n",
        "\n",
        "   plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
