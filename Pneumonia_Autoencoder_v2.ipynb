{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ub5cg7ZnzBn7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import gc\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAvgPool2D,Conv2D, MaxPool2D, BatchNormalization, Dropout, Input, LeakyReLU, Reshape, Conv2DTranspose, Concatenate, Lambda, MaxPooling2D\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        " \n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "UnKcRVzAz6CR",
        "outputId": "d34f283a-4527-4a01-9a4c-831bc0d35c44"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-94c949e0-0356-49ef-ad1a-25c74cd42259\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-94c949e0-0356-49ef-ad1a-25c74cd42259\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading chest-xray-pneumonia.zip to /content\n",
            "100% 2.29G/2.29G [01:46<00:00, 23.5MB/s]\n",
            "100% 2.29G/2.29G [01:46<00:00, 23.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Kaggle API Token\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Setup Kaggle API\n",
        "!pip install kaggle --quiet\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the Dataset\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        " # Extract the Dataset\n",
        "!unzip -q chest-xray-pneumonia.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the directory path for training and testing images\n",
        "base_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Set up the directory path for training and testing images\n",
        "base_dir = './chest_xray'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Create directories to combine images in the training and testing directories\n",
        "combined_dir = os.path.join(base_dir, 'combined')\n",
        "os.makedirs(combined_dir, exist_ok=True)\n",
        "\n",
        "# Create new 'NORMAL' and 'PNEUMONIA' directories inside the combined directory\n",
        "normal_dir = os.path.join(combined_dir, 'NORMAL')\n",
        "pneumonia_dir = os.path.join(combined_dir, 'PNEUMONIA')\n",
        "os.makedirs(normal_dir, exist_ok=True)\n",
        "os.makedirs(pneumonia_dir, exist_ok=True)\n",
        "\n",
        "# Function to copy files from source directories to a target directory\n",
        "def copy_files(source_dir, target_dir):\n",
        "   count = 0\n",
        "   for filename in os.listdir(source_dir):\n",
        "      file_path = os.path.join(source_dir, filename)\n",
        "      if os.path.isfile(file_path):\n",
        "         count += 1\n",
        "         shutil.copy(file_path, target_dir)\n",
        "\n",
        "   print(f'Copied {count} files from {source_dir} to {target_dir}')\n",
        "# Copy all 'NORMAL' images from the training, validation, and test directories to the new 'NORMAL' directory\n",
        "copy_files(os.path.join(train_dir, 'NORMAL'), normal_dir)\n",
        "copy_files(os.path.join(val_dir, 'NORMAL'), normal_dir)\n",
        "copy_files(os.path.join(test_dir, 'NORMAL'), normal_dir)\n",
        "\n",
        "# Copy all 'PNEUMONIA' images from the training, validation, and test directories to the new 'PNEUMONIA' directory\n",
        "copy_files(os.path.join(train_dir, 'PNEUMONIA'), pneumonia_dir)\n",
        "copy_files(os.path.join(val_dir, 'PNEUMONIA'), pneumonia_dir)\n",
        "copy_files(os.path.join(test_dir, 'PNEUMONIA'), pneumonia_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the overall distribution of the training, validation, and test datasets using plotly\n",
        "\n",
        "# Create a DataFrame with the number of images in each category\n",
        "data = {\n",
        "    'Dataset': ['Training', 'Validation', 'Test', 'All'],\n",
        "    'NORMAL': [len(os.listdir(os.path.join(train_dir, 'NORMAL'))), len(os.listdir(os.path.join(val_dir, 'NORMAL'))), len(os.listdir(os.path.join(test_dir, 'NORMAL'))), len(os.listdir(normal_dir))],\n",
        "    'PNEUMONIA': [len(os.listdir(os.path.join(train_dir, 'PNEUMONIA'))), len(os.listdir(os.path.join(val_dir, 'PNEUMONIA'))), len(os.listdir(os.path.join(test_dir, 'PNEUMONIA'))), len(os.listdir(pneumonia_dir))]\n",
        "}\n",
        "\n",
        "# Plot the distribution of images in each category\n",
        "fig = px.bar(pd.DataFrame(data).melt(id_vars='Dataset'), x='Dataset', y='value', color='variable', barmode='group')\n",
        "fig.update_layout(title='Distribution of Images in Each Category', xaxis_title='Dataset', yaxis_title='Number of Images')\n",
        "fig.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "# Preprocessing the images - setting up image data generators\n",
        "batch_size = 24\n",
        "image_size = 224\n",
        "color_ch = 3\n",
        "\n",
        "# Limited data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,\n",
        "                     width_shift_range=0.2, height_shift_range=0.2)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "combined_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(image_size, image_size),\n",
        "                                       batch_size=batch_size, class_mode='binary')\n",
        "val_generator = val_datagen.flow_from_directory(test_dir, target_size=(image_size, image_size),\n",
        "                                     batch_size=batch_size, class_mode='binary')\n",
        "test_generator = test_datagen.flow_from_directory(val_dir, target_size=(image_size, image_size),\n",
        "                                      batch_size=batch_size, class_mode='binary')\n",
        "combined_generator = combined_datagen.flow_from_directory(combined_dir, target_size=(image_size, image_size),\n",
        "                                       batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "\n",
        "# Use tf.data.Dataset for efficient data loading\n",
        "def ae_wrapper(generator):\n",
        "   for x_batch, _ in generator:\n",
        "      #x_batch = tf.image.rgb_to_grayscale(x_batch) # Convert images to grayscale\n",
        "      x_batch = tf.cast(x_batch, tf.float32) # Convert images to float32\n",
        "      yield (x_batch, x_batch)\n",
        "\n",
        "def clf_wrapper(generator):\n",
        "   for x_batch, y_batch in generator:\n",
        "      #x_batch = tf.image.rgb_to_grayscale(x_batch)  # Convert images to grayscale\n",
        "      x_batch = tf.cast(x_batch, tf.float32) # Convert images to float32\n",
        "      yield (x_batch, y_batch)\n",
        "\n",
        "def create_dataset(generator, wrapperFunc, output, buffer_size=50):\n",
        "   return tf.data.Dataset.from_generator(\n",
        "      lambda: wrapperFunc(generator),\n",
        "      output_signature=output\n",
        "   ).shuffle(buffer_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Set the buffer size for shuffling the dataset\n",
        "buffer_size = 50\n",
        "# Define the output signature for the datasets (the ae and the clf have different signatures)\n",
        "ae_ds_output = (tf.TensorSpec(shape=(image_size, image_size, color_ch), dtype=tf.float32), tf.TensorSpec(shape=(image_size, image_size, color_ch), dtype=tf.float32))\n",
        "clf_ds_output = (tf.TensorSpec(shape=(image_size, image_size, color_ch), dtype=tf.float32), tf.TensorSpec(shape=(), dtype=tf.float32))\n",
        "\n",
        "ae_combined = tf.data.Dataset.from_generator(\n",
        "   lambda: ae_wrapper(combined_generator),\n",
        "   output_signature=ae_ds_output\n",
        ").shuffle(buffer_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "clf_train = create_dataset(\n",
        "   train_generator,\n",
        "   clf_wrapper,\n",
        "   clf_ds_output,\n",
        "   buffer_size)\n",
        "\n",
        "clf_val = create_dataset(\n",
        "   val_generator,\n",
        "   clf_wrapper,\n",
        "   clf_ds_output,\n",
        "   buffer_size)\n",
        "\n",
        "clf_test = create_dataset(\n",
        "   test_generator,\n",
        "   clf_wrapper,\n",
        "   clf_ds_output,\n",
        "   buffer_size)\n",
        "\n",
        "print(ae_combined.element_spec)\n",
        "print(clf_train.element_spec)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del train_generator\n",
        "del val_generator\n",
        "del test_generator\n",
        "del combined_generator\n",
        "del train_dataset\n",
        "del val_dataset\n",
        "del test_dataset\n",
        "del combined_dataset\n",
        "del train_datagen\n",
        "del val_datagen\n",
        "del test_datagen\n",
        "del combined_datagen\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reRUWXoRzBn9",
        "outputId": "f9a30435-0738-4c20-8aea-b087fa8be347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Preprocessing the images - setting up image data generators\n",
        "batch_size = 24\n",
        "image_size = 224\n",
        "\n",
        "# Limited data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20,\n",
        "                           width_shift_range=0.2, height_shift_range=0.2)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "combined_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(image_size, image_size),\n",
        "                                       batch_size=batch_size, class_mode='binary')\n",
        "val_generator = val_datagen.flow_from_directory(test_dir, target_size=(image_size, image_size),\n",
        "                                     batch_size=batch_size, class_mode='binary')\n",
        "test_generator = test_datagen.flow_from_directory(val_dir, target_size=(image_size, image_size),\n",
        "                                      batch_size=batch_size, class_mode='binary')\n",
        "combined_generator = combined_datagen.flow_from_directory(combined_dir, target_size=(image_size, image_size),\n",
        "                                       batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "\n",
        "# Use tf.data.Dataset for efficient data loading\n",
        "def generator_wrapper(generator):\n",
        "   for x_batch, _ in generator:\n",
        "      x_batch = tf.cast(x_batch, tf.float32)\n",
        "      yield (x_batch, x_batch)\n",
        "# Shuffle buffer size\n",
        "buffer_size = 50\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "   lambda: generator_wrapper(train_generator),\n",
        "   output_signature=(\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32),\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32))\n",
        ").shuffle(buffer_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "   lambda: generator_wrapper(val_generator),\n",
        "   output_signature=(\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32),\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32))\n",
        ").shuffle(buffer_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "   lambda: generator_wrapper(test_generator),\n",
        "   output_signature=(\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32),\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32))\n",
        ").shuffle(buffer_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "combined_dataset = tf.data.Dataset.from_generator(\n",
        "   lambda: generator_wrapper(combined_generator),\n",
        "   output_signature=(\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32),\n",
        "      tf.TensorSpec(shape=(None, image_size, image_size, 3), dtype=tf.float32))\n",
        ").shuffle(buffer_size).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = next(train_generator)[0][0]\n",
        "plt.imshow(img)\n",
        "img = tf.image.rgb_to_grayscale(img)\n",
        "plt.imshow(img[:, :, 0], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZPqjnbSAId38"
      },
      "outputs": [],
      "source": [
        "# Define the callbacks\n",
        "\n",
        "ae_early_stopping = EarlyStopping(monitor='val_mean_squared_error', patience=3, verbose=1, mode='min', restore_best_weights=True)\n",
        "ae_model_checkpoint = ModelCheckpoint('conv_ae.keras', monitor='val_mean_squared_error', verbose=1,mode='min', save_best_only=True)\n",
        "ae_reduce_lr = ReduceLROnPlateau(monitor='val_mean_squared_error', factor=0.01, patience=4, verbose=1, mode='min')\n",
        "\n",
        "# Define the callbacks for the classifier (monitor validation accuracy)\n",
        "classifier_early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, verbose=1, mode='max', restore_best_weights=True)\n",
        "classifier_model_checkpoint = ModelCheckpoint('enhanced_clf_v1.keras', monitor='val_accuracy', verbose=1, mode='max', save_best_only=True)\n",
        "classifier_reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.01, patience=4, verbose=1, mode='max')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Encoder\n",
        "conv_encoder = Sequential([\n",
        "    Reshape([image_size, image_size, 3], name=\"input\"), # input: 224x224x1\n",
        "    BatchNormalization(),\n",
        "    Conv2D(16, 3, padding=\"same\", activation=\"relu\"), \n",
        "    MaxPool2D(pool_size=2), # output: 112x112x16\n",
        "    Dropout(0.15),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "    MaxPool2D(pool_size=2), # output: 56x56x32\n",
        "    Dropout(0.35),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "    MaxPool2D(pool_size=2), # output: 28x28x64\n",
        "    Dropout(0.55),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(128, 5, padding=\"same\", activation=\"relu\"),\n",
        "    MaxPool2D(pool_size=2), # output: 14x14x128\n",
        "    Dropout(0.35),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(256, 5, padding=\"same\", activation=\"relu\"),\n",
        "    MaxPool2D(pool_size=2), # output: 7x7x256\n",
        "    Dropout(0.15),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(512, 5, padding=\"same\", activation=\"relu\"),\n",
        "    MaxPool2D(pool_size=2), # output: 3x3x512\n",
        "    Dropout(0.15),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(60, 3, padding=\"same\", activation=\"relu\"),\n",
        "    GlobalAvgPool2D() # output: 60\n",
        "])\n",
        "\n",
        "# Decoder\n",
        "conv_decoder = Sequential([\n",
        "    Dense(28 * 28 * 512),\n",
        "    Reshape((28, 28, 512), name=\"decoder_input\"), # input: 28x28x512\n",
        "    Conv2DTranspose(512, 5, strides=2, padding=\"same\", activation=\"relu\"), # output: 56x56x512\n",
        "    Dropout(0.5),\n",
        "    BatchNormalization(),\n",
        "    Conv2DTranspose(256, 5, strides=2, padding=\"same\", activation=\"relu\"), # output: 112x112x256\n",
        "    Dropout(0.25),\n",
        "    BatchNormalization(),\n",
        "    Conv2DTranspose(128, 5, strides=2, padding=\"same\", activation=\"relu\"), # output: 224x224x128\n",
        "    Dropout(0.5),\n",
        "    BatchNormalization(),\n",
        "    Conv2DTranspose(64, 3, strides=1, padding=\"same\", activation=\"relu\"), # output: 224x224x64\n",
        "    Dropout(0.25),\n",
        "    BatchNormalization(),\n",
        "    Conv2DTranspose(32, 3, strides=1, padding=\"same\", activation=\"relu\"), # output: 224x224x32\n",
        "    Dropout(0.15),\n",
        "    BatchNormalization(),\n",
        "    Conv2DTranspose(16, 3, strides=1, padding=\"same\", activation=\"relu\"), # output: 224x224x16\n",
        "    Dropout(0.05),\n",
        "    BatchNormalization(),\n",
        "    Conv2DTranspose(3, 3, strides=1, padding=\"same\", activation=\"sigmoid\"), # output: 224x224x1\n",
        "    Reshape([image_size, image_size, 3], name=\"output\")\n",
        "])\n",
        "\n",
        "# Autoencoder\n",
        "conv_ae = Sequential([conv_encoder, conv_decoder])\n",
        "\n",
        "\n",
        "conv_ae.compile(\n",
        "    loss=\"mse\", \n",
        "    # Use nadam with a 0.01 learning rate\n",
        "    optimizer=tf.keras.optimizers.Nadam(learning_rate=0.1),\n",
        "     metrics=[tf.keras.metrics.MeanSquaredError(),'accuracy'])\n",
        "\n",
        "conv_ae.build(input_shape=(None, image_size, image_size, 3))\n",
        "conv_ae.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.debugging.disable_traceback_filtering()\n",
        "# reset keras backend\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function to build and fit models with different bottleneck sizes\n",
        "def build_and_fit_ae(bottleneck_range=[60, 200], bottleneck_step=20):\n",
        "   for i in range(bottleneck_range[0], bottleneck_range[1], bottleneck_step):\n",
        "      # Set a seed\n",
        "      tf.random.set_seed(1)\n",
        "      print(f\"Training model with bottleneck size {i}\")\n",
        "      tf.keras.backend.clear_session()\n",
        "      # Delete the model\n",
        "      if 'ae' in locals():\n",
        "         print('Deleting AE')\n",
        "         del ae\n",
        "      # Clear the cache\n",
        "      gc.collect()\n",
        "\n",
        "      # Encoder\n",
        "      encoder = Sequential([\n",
        "         Reshape([image_size, image_size, 3], name=\"input\"), # input: 224x224x1\n",
        "         BatchNormalization(),\n",
        "         Conv2D(16, 3, padding=\"same\", activation=\"relu\"), \n",
        "         MaxPool2D(pool_size=2), # output: 112x112x16\n",
        "         Dropout(0.15),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 56x56x32\n",
        "         Dropout(0.35),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 28x28x64\n",
        "         Dropout(0.55),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(128, 5, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 14x14x128\n",
        "         Dropout(0.35),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(256, 5, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 7x7x256\n",
        "         Dropout(0.15),\n",
        "         BatchNormalization(),\n",
        "         Conv2D(512, 5, padding=\"same\", activation=\"relu\"),\n",
        "         MaxPool2D(pool_size=2), # output: 3x3x512\n",
        "         Dropout(0.15),\n",
        "         BatchNormalization(),\n",
        "      ])\n",
        "\n",
        "      # Decoder\n",
        "      decoder = Sequential([\n",
        "         Dense(28 * 28 * 512),\n",
        "         Reshape((28, 28, 512), name=\"decoder_input\"), # input: 28x28x512\n",
        "         Conv2DTranspose(512, 5, strides=2, padding=\"same\", activation=\"relu\"), # output: 56x56x512\n",
        "         Dropout(0.5),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(256, 5, strides=2, padding=\"same\", activation=\"relu\"), # output: 112x112x256\n",
        "         Dropout(0.25),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(128, 5, strides=2, padding=\"same\", activation=\"relu\"), # output: 224x224x128\n",
        "         Dropout(0.5),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(64, 3, strides=1, padding=\"same\", activation=\"relu\"), # output: 224x224x64\n",
        "         Dropout(0.25),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(32, 3, strides=1, padding=\"same\", activation=\"relu\"), # output: 224x224x32\n",
        "         Dropout(0.15),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(16, 3, strides=1, padding=\"same\", activation=\"relu\"), # output: 224x224x16\n",
        "         Dropout(0.05),\n",
        "         BatchNormalization(),\n",
        "         Conv2DTranspose(3, 3, strides=1, padding=\"same\", activation=\"sigmoid\"), # output: 224x224x1\n",
        "         Reshape([image_size, image_size, 3], name=\"output\")\n",
        "      ])\n",
        "\n",
        "      encoder.add(Conv2D(i, 3, padding=\"same\", activation=\"relu\", kernel_initializer=GlorotUniform(seed=i)))\n",
        "      encoder.add(GlobalAvgPool2D())\n",
        "      \n",
        "      ae = Sequential([encoder, decoder], name=f\"conv_ae_BN{i}\")\n",
        "      ae.compile(\n",
        "         loss=\"mse\", \n",
        "         optimizer=tf.keras.optimizers.Nadam(learning_rate=0.05),\n",
        "         metrics=[tf.keras.metrics.MeanSquaredError(),'accuracy'])\n",
        "      \n",
        "      ae.build(input_shape=(None, image_size, image_size, 3))\n",
        "  \n",
        "    \n",
        "      histories = []\n",
        "      \n",
        "      history = ae.fit(\n",
        "         combined_dataset,\n",
        "         epochs=5,\n",
        "         steps_per_epoch=len(combined_generator)//10,\n",
        "      )\n",
        "      print('-----------------------------------------------------------------')\n",
        "      histories.append(history)\n",
        "   return histories\n",
        "\n",
        "bottleneck_test = build_and_fit_ae(bottleneck_range=[30, 110], bottleneck_step=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MNIST AE example from the Hands-On Machine Learning text reduced the input rank from 28x28x1 = 784 to 30, a 96.17% reduction. We are attempting to reduce our input rank from 224x224x3 = 150,528 to around 100, a 99.93% reduction. To make that difference more apparent, if we were to reduce rank by 96.17% on our input, we would have a bottleneck tensor rank of 5765 (150,528 * 0.0383). This is a significant difference in the amount of information we are trying to compress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Plot the histories of the autoencoder models on the same plot (superimposed) using plotly\n",
        "fig = go.Figure()\n",
        "bottleneck_sizes = np.arange(30, 110, 10)\n",
        "# Find the loss of the models at the final epoch\n",
        "min_loss = np.inf\n",
        "min_loss_idx = 0\n",
        "max_acc = 0\n",
        "max_acc_idx = 0\n",
        "for i, history in enumerate(bottleneck_test):\n",
        "   if history.history['loss'][-1] < min_loss:\n",
        "      min_loss = history.history['loss'][-1]\n",
        "      min_loss_idx = i\n",
        "   if history.history['accuracy'][-1] > max_acc:\n",
        "      max_acc = history.history['accuracy'][-1]\n",
        "      max_acc_idx = i\n",
        "\n",
        "\n",
        "for i, history in enumerate(bottleneck_test):\n",
        "   fig.add_trace(go.Scatter(x=np.arange(1, 6), y=history.history['loss'], mode='lines', name=f'Filters {bottleneck_sizes[i]}'))\n",
        "fig.update_layout(title='Autoencoder Training Loss for Different Bottleneck Sizes (Batch size: 24, steps-per-epoch: 24 [1/10th of all images])', xaxis_title='Epoch', yaxis_title='Loss')\n",
        "# Add an annotation to show the best model\n",
        "fig.add_annotation(x=5, ax=-100, y=min_loss, ay=-100, text=f'Min loss: {np.round(min_loss, 4)} @ {bottleneck_sizes[min_loss_idx]} filters', showarrow=True, arrowhead=1, font=dict(family=\"Courier New, monospace\", size=16, color=\"#ffffff\"), bordercolor=\"#c7c7c7\", borderwidth=2, borderpad=4, bgcolor=\"black\", opacity=0.8)\n",
        "fig.show()\n",
        "\n",
        "fig2 = go.Figure()\n",
        "for i, history in enumerate(bottleneck_test):\n",
        "   fig2.add_trace(go.Scatter(x=np.arange(1, 6), y=history.history['accuracy'], mode='lines', name=f'Filters {bottleneck_sizes[i]}'))\n",
        "fig2.update_layout(title='Autoencoder Training Accuracy for Different Bottleneck Sizes (Batch size: 24, steps-per-epoch: 24 [1/10th of all images])', xaxis_title='Epoch', yaxis_title='Accuracy')\n",
        "fig2.add_annotation(x=5, ax=-100, y=max_acc, ay=-20, text=f'Max accuracy: {np.round(max_acc, 4)} @ {bottleneck_sizes[max_acc_idx]} filters', showarrow=True, arrowhead=1, font=dict(family=\"Courier New, monospace\", size=16, color=\"#ffffff\"), bordercolor=\"#c7c7c7\", borderwidth=2, borderpad=4, bgcolor=\"black\", opacity=0.8)\n",
        "fig2.show()\n",
        "\n",
        "# Plot the variances in the loss for each model using a bar chart\n",
        "loss_variances = [np.var(history.history['loss']) for history in bottleneck_test]\n",
        "fig3 = go.Figure()\n",
        "fig3.add_trace(go.Bar(x=bottleneck_sizes, y=loss_variances, name='Loss Variance'))\n",
        "fig3.update_layout(title='Variance in Loss for Different Bottleneck Sizes (Batch size: 24, steps-per-epoch: 24 [1/10th of all images])', xaxis_title='Bottleneck Size (filters)', yaxis_title='Variance')\n",
        "# Annotate the top of the highest and lowest bar with their variances\n",
        "fig3.add_annotation(x=bottleneck_sizes[np.argmax(loss_variances)], y=np.max(loss_variances), text=f'{np.round(np.max(loss_variances), 4)}', showarrow=False, arrowhead=1, font=dict(family=\"Courier New, monospace\", size=16, color=\"#ffffff\"), bordercolor=\"#c7c7c7\", borderwidth=2, borderpad=4, bgcolor=\"black\", opacity=0.8)\n",
        "fig3.add_annotation(x=bottleneck_sizes[np.argmin(loss_variances)], y=np.min(loss_variances), text=f'{np.round(np.min(loss_variances), 4)}', showarrow=False, arrowhead=1, font=dict(family=\"Courier New, monospace\", size=16, color=\"#ffffff\"), bordercolor=\"#c7c7c7\", borderwidth=2, borderpad=4, bgcolor=\"black\", opacity=0.8)\n",
        "fig3.show()\n",
        "\n",
        "# Plot the variances in the accuracy for each model using a bar chart\n",
        "accuracy_variances = [np.var(history.history['accuracy']) for history in bottleneck_test]\n",
        "fig4 = go.Figure()\n",
        "fig4.add_trace(go.Bar(x=bottleneck_sizes, y=accuracy_variances, name='Accuracy Variance'))\n",
        "fig4.update_layout(title='Variance in Accuracy for Different Bottleneck Sizes (Batch size: 24, steps-per-epoch: 24 [1/10th of all images])', xaxis_title='Bottleneck Size (filters)', yaxis_title='Variance')\n",
        "# Annotate the top of the highest and lowest bar with their variances\n",
        "fig4.add_annotation(x=bottleneck_sizes[np.argmax(accuracy_variances)], y=np.max(accuracy_variances), text=f'{np.round(np.max(accuracy_variances), 4)}', showarrow=False, arrowhead=1, font=dict(family=\"Courier New, monospace\", size=16, color=\"#ffffff\"), bordercolor=\"#c7c7c7\", borderwidth=2, borderpad=4, bgcolor=\"black\", opacity=0.8)\n",
        "fig4.add_annotation(x=bottleneck_sizes[np.argmin(accuracy_variances)], y=np.min(accuracy_variances), text=f'{np.round(np.min(accuracy_variances), 4)}', showarrow=False, arrowhead=1, font=dict(family=\"Courier New, monospace\", size=16, color=\"#ffffff\"), bordercolor=\"#c7c7c7\", borderwidth=2, borderpad=4, bgcolor=\"black\", opacity=0.8)\n",
        "fig4.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the autoencoder\n",
        "history = conv_ae.fit(\n",
        "   combined_dataset,\n",
        "   epochs=30,\n",
        "   validation_data=val_dataset,\n",
        "   steps_per_epoch=(len(train_generator) + len(val_generator)),\n",
        "   validation_steps=len(val_generator),\n",
        "   callbacks=[\n",
        "      ae_early_stopping,\n",
        "      ae_model_checkpoint,\n",
        "      ae_reduce_lr\n",
        "   ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the training history using plotly\n",
        "\n",
        "def plot_history(history):\n",
        "   # Plot the mean squared error and accuracy\n",
        "   fig = px.line(\n",
        "      history.history,\n",
        "      y=['mean_squared_error', 'accuracy'],\n",
        "      labels={'index': 'epoch', 'value': 'mean squared error'},\n",
        "      title='Autoencoder Training History'\n",
        "   )\n",
        "   fig.show()\n",
        "\n",
        "plot_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to visualize the encoded images\n",
        "def visualize(img, encoder, decoder):\n",
        "   encoded = encoder.predict(img[None])[0]\n",
        "   decoded = decoder.predict(encoded[None])[0]\n",
        "   plt.figure(figsize=(10, 5))\n",
        "   plt.subplot(1, 3, 1)\n",
        "   plt.title(\"Original\")\n",
        "   plt.imshow(img)\n",
        "   plt.axis('off')\n",
        "   plt.subplot(1, 3, 2)\n",
        "   plt.title(\"Encoded\")\n",
        "   plt.plot(encoded)\n",
        "   plt.axis('off')\n",
        "   plt.subplot(1, 3, 3)\n",
        "   plt.title(\"Decoded\")\n",
        "   plt.imshow(decoded)\n",
        "   plt.axis('off')\n",
        "   plt.show()\n",
        "\n",
        "# Visualize the images from the val_generator\n",
        "for i in range(10):\n",
        "   img = next(val_generator)[0][0]\n",
        "   visualize(img, conv_encoder, conv_decoder)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Pneumonia Detection using Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the classifier as a Functional model, including a block of convolutional layers, then the encoder, then a block of dense layers\n",
        "def build_classifier(encoder, weights={'vanilla': 0.5, 'encoder': 0.5}):\n",
        "   # Freeze the encoder layers\n",
        "   for layer in encoder.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "   # Create an input layer that contains two inputs: the encoder input and a shallow input\n",
        "   input_= Input((image_size, image_size, 3), name='input')\n",
        "\n",
        "\n",
        "   # Functional model, including a block of convolutional layers in tandem with the encoder (add one fully connected layer to learn the features anew), then a block of dense layers\n",
        "   # Two paths: one for the vanilla input, one for the encoder output\n",
        "\n",
        "   # Vanilla input\n",
        "   x = Conv2D(16, (1, 1), strides=1, activation='relu', name='block_1_conv_1')(input_) # Output: 224x224x16\n",
        "   x = Dropout(0.25, name='block_1_dropout')(x)\n",
        "   x = BatchNormalization(name='block_1_batchNorm_1')(x)\n",
        "   x = Conv2D(32, (3, 3), strides=2, activation='relu', name='block_1_conv_2')(x) # Output: 112x112x32\n",
        "   x = Dropout(0.35, name='block_1_dropout_2')(x)\n",
        "   x = BatchNormalization(name='block_1_batchNorm_2')(x)\n",
        "\n",
        "   x = Conv2D(64, (3, 3), activation='relu', name='block_2_conv_1')(x) # Output: 112x112x64 \n",
        "   x = Dropout(0.5, name='block_2_dropout_1')(x) \n",
        "   x = BatchNormalization(name='block_2_batchNorm_1')(x)    \n",
        "   x = Conv2D(128, (3, 3), strides=2, activation='relu', name='block_2_conv_2')(x) # Output: 56x56x128\n",
        "   x = Dropout(0.35, name='block_2_dropout_2')(x)\n",
        "   x = BatchNormalization(name='block_2_batchNorm_2')(x)\n",
        "\n",
        "   x = Conv2D(256, (5, 5), activation='relu', name='block_3_conv_1')(x) # Output: 56x56x256\n",
        "   x = Dropout(0.15, name='block_3_dropout_1')(x)\n",
        "   x = BatchNormalization(name='block_3_batchNorm_1')(x)   \n",
        "   x = Conv2D(512, (5, 5), strides=2, activation='relu', name='block_3_conv_2')(x) # Output: 28x28x512\n",
        "   x = Dropout(0.05, name='block_3_dropout_2')(x)\n",
        "   x = BatchNormalization(name='block_3_batchNorm_2')(x)   \n",
        "   x = GlobalAvgPool2D(name=\"block_4_globalAvgPool2D\")(x) # Output: 512\n",
        "   # Weight the vanilla output\n",
        "   vanilla_output = Lambda(lambda x: x * weights['vanilla'], name=\"vanilla_weighted_output\")(x) # Output: 512\n",
        "\n",
        "   # Encoder \n",
        "   encoded_features = encoder(input_)\n",
        "   x = BatchNormalization(name=\"encoder_batchNorm_1\")(encoded_features)\n",
        "   # Weight the encoder output\n",
        "   encoder_output = Lambda(lambda x: x * weights['encoder'], name=\"encoder_weighted_output\")(x) # Output: 60\n",
        " \n",
        "   # average the two outputs\n",
        "   DNN_output = layers.Average(name='average')([vanilla_output, encoder_output])\n",
        "\n",
        "   # Create the model\n",
        "   model = Model(inputs=input_, outputs=DNN_output)\n",
        "\n",
        "   return model\n",
        "\n",
        "# Build the classifier\n",
        "van_weight = 0.9\n",
        "enc_weight = 0.1\n",
        "classifier = build_classifier(conv_encoder, weights={'vanilla': 0.8, 'encoder': 0.2})\n",
        "classifier.compile(\n",
        "   loss='binary_crossentropy',\n",
        "   optimizer=tf.keras.optimizers.Nadam(learning_rate=0.01),\n",
        "   metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "# SHow the block diagram of the classifier using plot_model\n",
        "plot_model(classifier, show_shapes=True, show_layer_names=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Concatenate the weighted inputs\n",
        "\n",
        "   combined_output = Concatenate(name=\"combined_output\")([vanilla_output, encoder_output])\n",
        "   x = BatchNormalization(name=\"combined_batchNorm_1\")(combined_output)\n",
        "   x = Dense(256, activation='relu', kernel_initializer=\"he_normal\", name=\"combined_dense_1\")(x)\n",
        "   x = Dropout(0.5, name=\"combined_dropout_1\")(x)\n",
        "   x = BatchNormalization(name=\"combined_batchNorm_2\")(x)\n",
        "   x = Dense(128, activation='relu', kernel_initializer=\"he_normal\", name=\"combined_dense_2\")(x)\n",
        "   x = Dropout(0.3, name=\"combined_dropout_2\")(x)\n",
        "   x = BatchNormalization(name=\"combined_batchNorm_3\")(x)\n",
        "   x = Dense(64, activation='relu', kernel_initializer=\"he_normal\", name=\"combined_dense_3\")(x)\n",
        "   x = Dropout(0.1, name=\"combined_dropout_3\")(x)\n",
        "   x = BatchNormalization(name=\"combined_batchNorm_4\")(x)\n",
        "   x = Dense(32, activation='relu', kernel_initializer=\"he_normal\", name=\"combined_dense_4\")(x)\n",
        "   x = Dropout(0.1, name=\"combined_dropout_4\")(x)\n",
        "   x = BatchNormalization(name=\"combined_batchNorm_5\")(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to build and fit models with different weights for the vanilla and encoder inputs\n",
        "\n",
        "def build_and_fit_classifier():\n",
        "   # For loop to train and evaluate different weight splits\n",
        "   iterations=11\n",
        "   scores = []\n",
        "   for i in range(iterations):\n",
        "      print(f\"Training model {i+1}/{iterations}\")\n",
        "      # Define the weights\n",
        "      weights = {\n",
        "         'vanilla': 0.1 * i,\n",
        "         'encoder': 1 - (0.1 * i)\n",
        "      }\n",
        "      print(f'Weights: {weights}')\n",
        "\n",
        "      # Build the classifier\n",
        "      classifier = build_classifier(conv_encoder, weights=weights)\n",
        "      classifier.compile(\n",
        "         loss='binary_crossentropy',\n",
        "         optimizer=tf.keras.optimizers.Nadam(learning_rate=0.01),\n",
        "         metrics=['accuracy', 'AUC']\n",
        "      )\n",
        "\n",
        "      # Train the classifier\n",
        "      history = classifier.fit(\n",
        "         train_generator,\n",
        "         epochs=10,\n",
        "         validation_data=val_generator,\n",
        "         steps_per_epoch=len(train_generator)//10,\n",
        "         validation_steps=len(val_generator)//10,\n",
        "         callbacks=[\n",
        "            classifier_early_stopping,\n",
        "            classifier_model_checkpoint,\n",
        "            classifier_reduce_lr\n",
        "         ]\n",
        "      )\n",
        "\n",
        "      # Evaluate the classifier\n",
        "      scores.append(classifier.evaluate(test_generator, return_dict=True))\n",
        "      \n",
        "   return scores, history\n",
        "\n",
        "# Build and fit the classifier\n",
        "scores,best_history = build_and_fit_classifier()\n",
        "\n",
        "# Plot the highest scores using plotly\n",
        "\n",
        "def plot_scores(scores):\n",
        "      # Plot the accuracy and AUC\n",
        "      fig = px.bar(\n",
        "         scores,\n",
        "         y=['accuracy', 'auc'],\n",
        "         labels={'index': 'model', 'value': 'score'},\n",
        "         title='Classifier Scores'\n",
        "      )\n",
        "      # annotate the weights for each model\n",
        "      fig.update_layout(\n",
        "         xaxis=dict(tickvals=list(range(len(scores))),\n",
        "                     ticktext=[f'Weights: 0.{i} / 0.{10-i}' for i in range(11)]\n",
        "         )\n",
        "      )\n",
        "      fig.show()\n",
        "\n",
        "plot_scores(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the classifier\n",
        "history = classifier.fit(\n",
        "   # train on just 100 images\n",
        "   train_generator,\n",
        "   validation_data=val_generator,\n",
        "   epochs=15,\n",
        "   steps_per_epoch=len(train_generator)//4,\n",
        "   validation_steps=len(val_generator)//4,\n",
        "   callbacks=[classifier_early_stopping, classifier_model_checkpoint, classifier_reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the classifier\n",
        "#classifier.evaluate(val_generator, steps=len(val_generator))\n",
        "\n",
        "# Get the true labels\n",
        "y_true = val_generator.classes\n",
        "# Get the predicted labels\n",
        "y_pred = classifier.predict(val_generator).argmax(axis=1)\n",
        "# Round the predicted labels up and down\n",
        "y_pred = np.round(y_pred)\n",
        "# Print the classification report\n",
        "print(classification_report(y_true, y_pred, target_names=val_generator.class_indices.keys()))\n",
        "\n",
        "# Plot the training history using plotly\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines', name='Train'))\n",
        "fig.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines', name='Validation'))\n",
        "fig.add_trace(go.Scatter(y=history.history['auc'], mode='lines', name='AUC'))\n",
        "fig.update_layout(title=f'Accuracy<br> (vanilla path: {van_weight})', xaxis_title='Epochs', yaxis_title='Accuracy')\n",
        "fig.show()\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=history.history['loss'], mode='lines', name='Train'))\n",
        "fig.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines', name='Validation'))\n",
        "fig.update_layout(title='Loss', xaxis_title='Epochs', yaxis_title='Loss')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the predictions images and labels from the classifier\n",
        "predictions = classifier.predict(val_generator, steps=len(val_generator))\n",
        "example_true_labels = val_generator.classes\n",
        "for i in range(5):\n",
        "   img = next(val_generator)[0][0]\n",
        "   plt.imshow(img)\n",
        "   plt.title(f'Prediction: {predictions[i]}, True Label: {example_true_labels[i]}')\n",
        "\n",
        "   plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Denoising Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "denoising_encoder = tf.keras.Sequential([\n",
        "   tf.keras.layers.GaussianNoise(0.1),\n",
        "   tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
        "   tf.keras.layers.MaxPool2D(),\n",
        "   tf.keras.layers.Flatten(),\n",
        "   tf.keras.layers.Dense(512, activation=\"relu\"),\n",
        "])\n",
        "denoising_decoder = tf.keras.Sequential([\n",
        "   tf.keras.layers.Dense(79 * 79 * 3, activation=\"relu\"),  # Adjusted for 224x224x3 images\n",
        "   tf.keras.layers.Reshape([79, 79, 3]),  # Adjusted for 224x224x3 images\n",
        "   tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=2,\n",
        "                         padding=\"same\", activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "denoising_ae = tf.keras.Sequential([denoising_encoder, denoising_decoder])\n",
        "denoising_ae.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "                metrics=[\"mse\",  \"accuracy\"])\n",
        "history = denoising_ae.fit(train_dataset, \n",
        "                           epochs=10,\n",
        "                           validation_data=test_dataset,\n",
        "                           callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
        "                           verbose=1,\n",
        "                           steps_per_epoch=len(train_generator),\n",
        "                           validation_steps=len(test_generator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8oChzaEJCDF"
      },
      "outputs": [],
      "source": [
        "n_images = 5\n",
        "new_images = val_dataset.take(n_images)\n",
        "# Get the image data for the first image\n",
        "new_images = next(iter(new_images))[0]\n",
        "# Add some noise to the images\n",
        "new_images_noisy = new_images + np.random.normal(loc=0.0, scale=0.1, size=new_images.shape)\n",
        "new_images_denoised = denoising_ae.predict(new_images_noisy)\n",
        "\n",
        "# Plot the images\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n_images):\n",
        "   plt.subplot(3, n_images, i + 1)\n",
        "   plt.imshow(new_images[i])\n",
        "   plt.axis('off')\n",
        "   plt.subplot(3, n_images, i + 1 + n_images)\n",
        "   plt.imshow(new_images_noisy[i])\n",
        "   plt.axis('off')\n",
        "   plt.subplot(3, n_images, i + 1 + 2 * n_images)\n",
        "   plt.imshow(new_images_denoised[i])\n",
        "   plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
